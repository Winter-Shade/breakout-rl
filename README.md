# Breakout AI: Deep Reinforcement Learning with CNN + Frame Stacking

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)

An AI agent that learns to play Breakout from raw pixels using Deep Reinforcement Learning (PPO + CNN)

![Breakout Demo](breakout_gameplay.gif) 

## ğŸ¯ Project Overview

This project demonstrates how to train an AI agent to master the classic Breakout game using:
- **Convolutional Neural Networks (CNN)** for visual processing
- **Frame Stacking** (4 frames) to capture temporal information
- **Proximal Policy Optimization (PPO)** for stable training
- **Reward Shaping** to guide learning

---

## ğŸ“ Project Structure

```
breakout-rl/
â”œâ”€â”€ breakout_env.py          # Custom Breakout environment (84x84 grayscale)
â”œâ”€â”€ train_cnn.py             # Main training script with PPO
â”œâ”€â”€ visualize_agent.py       # Visualization tool for trained agents
â”œâ”€â”€ test_cnn_stable.py       # Testing and evaluation script
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md               # This file
â”‚
â”œâ”€â”€ checkpoints_cnn_stable/  # Training checkpoints (saved every 50k steps)
â”œâ”€â”€ best_model_cnn_stable/   # Best performing model
â”œâ”€â”€ tensorboard_cnn_stable/  # TensorBoard logs
â””â”€â”€ logs_cnn_stable/         # Evaluation logs
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/Winter-Shade/breakout-rl.git
cd breakout-rl

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Train the Agent

```bash
# Start training (2M timesteps, ~4-6 hours on GPU)
python train_cnn.py

# Monitor training in another terminal
tensorboard --logdir ./tensorboard_cnn_stable/
# Open: http://localhost:6006
```

### 3. Visualize the Trained Agent

```bash
# Watch the agent play 
python visualize_agent.py --model ppo_breakout_cnn_1200000_steps --episodes 5

# Or use the best model
python visualize_agent.py --model best_model_cnn_stable/best_model --episodes 10
```

### 4. Evaluate Performance

```bash
# Test on 50 episodes
python test_cnn_stable.py --model ppo_breakout_cnn_stable_final --episodes 50

# Compare multiple checkpoints
python test_cnn_stable.py --compare
```

---

## ğŸ—ï¸ Technical Architecture

### Environment Specifications

**State Space:**
- **Dimensions:** 84Ã—84Ã—1 (grayscale images)
- **Stacked Frames:** 4 consecutive frames
- **Total Input:** 4Ã—84Ã—84 = 28,224 pixels
- **Theoretical State Space Size:** 256^28,224 possible states

**Action Space:**
- **Size:** 3 discrete actions
  - 0: Stay (no movement)
  - 1: Move Left
  - 2: Move Right

**Observation Processing:**
1. Render game at 640Ã—480 (native resolution)
2. Convert RGB to grayscale (luminosity method)
3. Resize to 84Ã—84 (standard Atari preprocessing)
4. Stack 4 consecutive frames for temporal information

### Neural Network Architecture

```
Input: 4Ã—84Ã—84 (4 stacked grayscale frames)
    â†“
Conv1: 32 filters, 8Ã—8 kernel, stride 4 â†’ (32, 20, 20)
    â†“ ReLU
Conv2: 64 filters, 4Ã—4 kernel, stride 2 â†’ (64, 9, 9)
    â†“ ReLU
Conv3: 64 filters, 3Ã—3 kernel, stride 1 â†’ (64, 7, 7)
    â†“ ReLU + Flatten
Fully Connected: 512 units
    â†“
Split:
    â”œâ”€ Actor (Policy): 512 â†’ 3 actions
    â””â”€ Critic (Value): 512 â†’ 1 value estimate

```

### Reward Structure

| Event | Reward | Purpose |
|-------|--------|---------|
| Paddle Hit | +5.0 + streak bonus | Encourage ball control |
| Brick Destroyed | +1.0 | Learn objective |
| Ball Missed | -10.0 | Avoid losing |
| Game Won | +100.0 | Terminal goal |
| Distance Shaping | +0.1 | Guide paddle alignment |
| Survival | +0.01/step | Prevent giving up |

### Training Configuration

```python
Algorithm: PPO (Proximal Policy Optimization)
Learning Rate: 1e-4
Batch Size: 512
Buffer Size: 512 steps per update
Discount Factor (Î³): 0.99
GAE Lambda (Î»): 0.95
Clip Range (Îµ): 0.1
Entropy Coefficient: 0.01
Value Function Coefficient: 0.5
Gradient Clipping: 0.5
Reward Normalization: Enabled
```

---

## ğŸ“ Key Implementation Details

### 1. Frame Stacking

**Problem:** Single frames don't show motion
```python
observation = [4Ã—84Ã—84 images]  
```
Stack 4 consecutive frames to make trajectory visible
- Frame t-3, t-2, t-1, t (current)
- CNN learns motion patterns from frame differences

### 2. Reward Normalization

**Problem:** Rewards range from -10 to +100 â†’ Value function instability

**Solution:** Normalize rewards to mean â‰ˆ 0, std â‰ˆ 1
```python
normalized_reward = (reward - running_mean) / running_std
```

**Result:** Stable training, no catastrophic forgetting

### 3. Curriculum Learning

The environment implements progressive difficulty:
- **80% easy starts:** Ball near paddle, moving upward
- **20% hard starts:** Ball coming down from top
- **Speed increase:** Ball speeds up after 5+ consecutive hits

---


## ğŸ“ˆ Monitoring Training

### TensorBoard 

```bash
tensorboard --logdir ./tensorboard_cnn_stable/
```

**Key metrics to watch:**
- `rollout/ep_rew_mean`: Should steadily increase
- `train/value_loss`: Should decrease and stabilize
- `train/explained_variance`: Should be high (0.7-0.9)

---

## Experiments & Variations

### Try Different Reward Structures

```python
# In breakout_env.py, modify step() method:
base_reward = 3.0  # Change paddle reward
reward += 2.0      # Change brick reward
```

### Test Different Network Architectures

```python
policy_kwargs=dict(
    net_arch=[512, 512],     # Different hidden layers
    activation_fn=th.nn.Tanh,  # Different activation
)
```

---

## ğŸ“š References & Resources

### Papers
- [PPO (2017)](https://arxiv.org/abs/1707.06347) - Schulman et al., "Proximal Policy Optimization Algorithms"
- [DQN (2015)](https://www.nature.com/articles/nature14236) - Mnih et al., "Human-level control through deep reinforcement learning"

### Libraries & Frameworks
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- [Gymnasium](https://gymnasium.farama.org/) 
- [PyTorch](https://pytorch.org/) 

---




